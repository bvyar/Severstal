{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, df, batch_size = 10, subset = \"train\", shuffle = False, preprocess = None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.shuffle = shuffle\n",
    "        self.subset = subset\n",
    "        self.batch_size = batch_size\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "        \n",
    "        if (self.subset == \"train\") or (self.subset == \"val\"):\n",
    "            self.data_path = '../input/severstal-steel-defect-detection/train_images/'\n",
    "        elif self.subset == \"test\":\n",
    "            self.data_path = '../input/severstal-steel-defect-detection/test_images/'\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.df) / self.batch_size)) # возвращает количество батчей за эпоху\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.df)) # возвращает одномерный массив с равномерно разнесенными значениями внутри заданного интервала\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes) # перемешивает входной датасет df каждую эпоху\n",
    "    \n",
    "    def augment(self, images, masks):\n",
    "        data_gen_args = dict(horizontal_flip = True, vertical_flip = True)\n",
    "        image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "        mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "        seed = random.randint(1, 1000)\n",
    "        #image_datagen.fit(images, augment=False, rounds=1, seed=seed)\n",
    "        #mask_datagen.fit(masks, augment=False, rounds=1, seed=seed)\n",
    "        images_aug = image_datagen.flow(images, seed=seed, batch_size = self.batch_size)[0]\n",
    "        masks_aug = mask_datagen.flow(masks, seed=seed, batch_size = self.batch_size)[0]\n",
    "        return images_aug, masks_aug\n",
    "    \n",
    "    def augment1(self, img, mask):\n",
    "        rand = random.randint(1, 1000)\n",
    "        if(rand > 900): return cv2.flip(img, 0), cv2.flip(mask, 0) #отражает изображение по вертикали\n",
    "        if(rand < 100): return cv2.flip(img, 1), cv2.flip(mask, 1) #отражает изображение по горизонтали\n",
    "        else: return img, mask\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        images = np.empty((self.batch_size, 256, 1600, 3), dtype=np.float32) # массив, состоящий из batch_size изображений (256x1600x3)\n",
    "        masks = np.empty((self.batch_size, 256, 1600, 4), dtype=np.int8) # массив, состоящий из batch_size изображений с масками\n",
    "        indexes = self.indexes[index * self.batch_size:(index+1) * self.batch_size]\n",
    "        \n",
    "        for i, img_id in enumerate(self.df['ImageId'].iloc[indexes]):\n",
    "            img = cv2.imread(self.data_path + img_id) # цветовым пространством по умолчанию в OpenCV является BGR\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # чтобы исправить это, используется cvtColor(image, flag) и рассмотренный выше флаг\n",
    "            images[i,] = img.astype(np.float32) / 255.\n",
    "            if (self.subset == \"train\") or (self.subset == \"val\"): \n",
    "                for j in range(4):\n",
    "                    masks[i,:,:,j] = rle2mask(self.df['e' + str(j+1)].iloc[indexes[i]], img.shape) # 4 канала, нулевой канал - дефект первого типа, третий канал - дефект четвертого типа\n",
    "            if self.subset == \"train\":\n",
    "                images[i,], masks[i,] = self.augment1(images[i,], masks[i,])\n",
    "        if self.subset == 'train':\n",
    "            #images, masks = self.augment(images, masks)\n",
    "            return images, masks\n",
    "        if self.subset == 'val':\n",
    "            return images, masks\n",
    "        else: return images # если test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.optimizer_v2.optimizer_v2 import OptimizerV2\n",
    "from tensorflow.python import ops, math_ops, state_ops, control_flow_ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "__all__ = ['RAdam']\n",
    "\n",
    "\n",
    "class RAdam(OptimizerV2):\n",
    "    \"\"\"RAdam optimizer.\n",
    "    According to the paper\n",
    "    [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=0.001,\n",
    "                 beta_1=0.9,\n",
    "                 beta_2=0.999,\n",
    "                 epsilon=1e-7,\n",
    "                 weight_decay=0.,\n",
    "                 amsgrad=False,\n",
    "                 total_steps=0,\n",
    "                 warmup_proportion=0.1,\n",
    "                 min_lr=0.,\n",
    "                 name='RAdam',\n",
    "                 **kwargs):\n",
    "        r\"\"\"Construct a new Adam optimizer.\n",
    "        Args:\n",
    "            learning_rate: A Tensor or a floating point value.    The learning rate.\n",
    "            beta_1: A float value or a constant float tensor. The exponential decay\n",
    "                rate for the 1st moment estimates.\n",
    "            beta_2: A float value or a constant float tensor. The exponential decay\n",
    "                rate for the 2nd moment estimates.\n",
    "            epsilon: A small constant for numerical stability. This epsilon is\n",
    "                \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
    "                Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
    "            weight_decay: A floating point value. Weight decay for each param.\n",
    "            amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\n",
    "                the paper \"On the Convergence of Adam and beyond\".\n",
    "            total_steps: An integer. Total number of training steps.\n",
    "                Enable warmup by setting a positive value.\n",
    "            warmup_proportion: A floating point value. The proportion of increasing steps.\n",
    "            min_lr: A floating point value. Minimum learning rate after warmup.\n",
    "            name: Optional name for the operations created when applying gradients.\n",
    "                Defaults to \"Adam\".    @compatibility(eager) When eager execution is\n",
    "                enabled, `learning_rate`, `beta_1`, `beta_2`, and `epsilon` can each be\n",
    "                a callable that takes no arguments and returns the actual value to use.\n",
    "                This can be useful for changing these values across different\n",
    "                invocations of optimizer functions. @end_compatibility\n",
    "            **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n",
    "                `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n",
    "                gradients by value, `decay` is included for backward compatibility to\n",
    "                allow time inverse decay of learning rate. `lr` is included for backward\n",
    "                compatibility, recommended to use `learning_rate` instead.\n",
    "        \"\"\"\n",
    "\n",
    "        super(RAdam, self).__init__(name, **kwargs)\n",
    "        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n",
    "        self._set_hyper('beta_1', beta_1)\n",
    "        self._set_hyper('beta_2', beta_2)\n",
    "        self._set_hyper('decay', self._initial_decay)\n",
    "        self._set_hyper('weight_decay', weight_decay)\n",
    "        self._set_hyper('total_steps', float(total_steps))\n",
    "        self._set_hyper('warmup_proportion', warmup_proportion)\n",
    "        self._set_hyper('min_lr', min_lr)\n",
    "        self.epsilon = epsilon or K.epsilon()\n",
    "        self.amsgrad = amsgrad\n",
    "        self._initial_weight_decay = weight_decay\n",
    "        self._initial_total_steps = total_steps\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'm')\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'v')\n",
    "        if self.amsgrad:\n",
    "            for var in var_list:\n",
    "                self.add_slot(var, 'vhat')\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        params = self.weights\n",
    "        num_vars = int((len(params) - 1) / 2)\n",
    "        if len(weights) == 3 * num_vars + 1:\n",
    "            weights = weights[:len(params)]\n",
    "        super(RAdam, self).set_weights(weights)\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        m = self.get_slot(var, 'm')\n",
    "        v = self.get_slot(var, 'v')\n",
    "        beta_1_t = self._get_hyper('beta_1', var_dtype)\n",
    "        beta_2_t = self._get_hyper('beta_2', var_dtype)\n",
    "        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n",
    "        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n",
    "        beta_1_power = math_ops.pow(beta_1_t, local_step)\n",
    "        beta_2_power = math_ops.pow(beta_2_t, local_step)\n",
    "\n",
    "        if self._initial_total_steps > 0:\n",
    "            total_steps = self._get_hyper('total_steps', var_dtype)\n",
    "            warmup_steps = total_steps * self._get_hyper('warmup_proportion', var_dtype)\n",
    "            min_lr = self._get_hyper('min_lr', var_dtype)\n",
    "            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n",
    "            decay_rate = (min_lr - lr_t) / decay_steps\n",
    "            lr_t = tf.where(\n",
    "                local_step <= warmup_steps,\n",
    "                lr_t * (local_step / warmup_steps),\n",
    "                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n",
    "            )\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n",
    "\n",
    "        m_t = state_ops.assign(m,\n",
    "                               beta_1_t * m + (1.0 - beta_1_t) * grad,\n",
    "                               use_locking=self._use_locking)\n",
    "        m_corr_t = m_t / (1.0 - beta_1_power)\n",
    "\n",
    "        v_t = state_ops.assign(v,\n",
    "                               beta_2_t * v + (1.0 - beta_2_t) * math_ops.square(grad),\n",
    "                               use_locking=self._use_locking)\n",
    "        if self.amsgrad:\n",
    "            vhat = self.get_slot(var, 'vhat')\n",
    "            vhat_t = state_ops.assign(vhat,\n",
    "                                      math_ops.maximum(vhat, v_t),\n",
    "                                      use_locking=self._use_locking)\n",
    "            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta_2_power))\n",
    "        else:\n",
    "            vhat_t = None\n",
    "            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta_2_power))\n",
    "\n",
    "        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
    "                            (sma_t - 2.0) / (sma_inf - 2.0) *\n",
    "                            sma_inf / sma_t)\n",
    "\n",
    "        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n",
    "\n",
    "        if self._initial_weight_decay > 0.0:\n",
    "            var_t += self._get_hyper('weight_decay', var_dtype) * var\n",
    "\n",
    "        var_update = state_ops.assign_sub(var,\n",
    "                                          lr_t * var_t,\n",
    "                                          use_locking=self._use_locking)\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "        if self.amsgrad:\n",
    "            updates.append(vhat_t)\n",
    "        return control_flow_ops.group(*updates)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var, indices):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        beta_1_t = self._get_hyper('beta_1', var_dtype)\n",
    "        beta_2_t = self._get_hyper('beta_2', var_dtype)\n",
    "        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n",
    "        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n",
    "        beta_1_power = math_ops.pow(beta_1_t, local_step)\n",
    "        beta_2_power = math_ops.pow(beta_2_t, local_step)\n",
    "\n",
    "        if self._initial_total_steps > 0:\n",
    "            total_steps = self._get_hyper('total_steps', var_dtype)\n",
    "            warmup_steps = total_steps * self._get_hyper('warmup_proportion', var_dtype)\n",
    "            min_lr = self._get_hyper('min_lr', var_dtype)\n",
    "            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n",
    "            decay_rate = (min_lr - lr_t) / decay_steps\n",
    "            lr_t = tf.where(\n",
    "                local_step <= warmup_steps,\n",
    "                lr_t * (local_step / warmup_steps),\n",
    "                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n",
    "            )\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n",
    "\n",
    "        m = self.get_slot(var, 'm')\n",
    "        m_scaled_g_values = grad * (1 - beta_1_t)\n",
    "        m_t = state_ops.assign(m, m * beta_1_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([m_t]):\n",
    "            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n",
    "        m_corr_t = m_t / (1.0 - beta_1_power)\n",
    "\n",
    "        v = self.get_slot(var, 'v')\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta_2_t)\n",
    "        v_t = state_ops.assign(v, v * beta_2_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([v_t]):\n",
    "            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhat = self.get_slot(var, 'vhat')\n",
    "            vhat_t = state_ops.assign(vhat,\n",
    "                                      math_ops.maximum(vhat, v_t),\n",
    "                                      use_locking=self._use_locking)\n",
    "            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta_2_power))\n",
    "        else:\n",
    "            vhat_t = None\n",
    "            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta_2_power))\n",
    "\n",
    "        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
    "                            (sma_t - 2.0) / (sma_inf - 2.0) *\n",
    "                            sma_inf / sma_t)\n",
    "\n",
    "        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n",
    "\n",
    "        if self._initial_weight_decay > 0.0:\n",
    "            var_t += self._get_hyper('weight_decay', var_dtype) * var\n",
    "\n",
    "        var_update = self._resource_scatter_add(var, indices, tf.gather(-lr_t * var_t, indices))\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "        if self.amsgrad:\n",
    "            updates.append(vhat_t)\n",
    "        return control_flow_ops.group(*updates)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(RAdam, self).get_config()\n",
    "        config.update({\n",
    "            'learning_rate': self._serialize_hyperparameter('learning_rate'),\n",
    "            'beta_1': self._serialize_hyperparameter('beta_1'),\n",
    "            'beta_2': self._serialize_hyperparameter('beta_2'),\n",
    "            'decay': self._serialize_hyperparameter('decay'),\n",
    "            'weight_decay': self._serialize_hyperparameter('weight_decay'),\n",
    "            'epsilon': self.epsilon,\n",
    "            'amsgrad': self.amsgrad,\n",
    "            'total_steps': self._serialize_hyperparameter('total_steps'),\n",
    "            'warmup_proportion': self._serialize_hyperparameter('warmup_proportion'),\n",
    "            'min_lr': self._serialize_hyperparameter('min_lr'),\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance_loss(y_true, y_pred, smooth = 1):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    \n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    \n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    #tf.to_float = lambda x: tf.cast(x, tf.float32)\n",
    "    #y_pred = tf.to_float(y_pred > 0.45)\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dice_Coef(y_true, y_pred, smooth = 1):\n",
    "    \n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    \n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    \n",
    "    return (2*intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def Dice_Loss(y_true, y_pred):\n",
    "    return 1.0 - Dice_Coef(y_true, y_pred)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return keras.losses.binary_crossentropy(y_true, y_pred) + Dice_Loss(y_true, y_pred)\n",
    "\n",
    "def wbce_dice_loss(y_true, y_pred):\n",
    "    return weighted_bce()(y_true, y_pred) + Dice_Loss(y_true, y_pred)\n",
    "\n",
    "def weighted_bce(weight = 0.6):\n",
    "    \n",
    "    def convert_2_logits(y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        return tf.math.log(y_pred / (1-y_pred))\n",
    "    \n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "        y_pred = convert_2_logits(y_pred)\n",
    "        loss = tf.nn.weighted_cross_entropy_with_logits(logits = y_pred, labels = y_true, pos_weight = weight)\n",
    "        return loss\n",
    "    \n",
    "    return weighted_binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('../input/pre-model10/256_resnet34 0.908.h5', custom_objects = {'RAdam': RAdam, 'dice_coef': dice_coef, 'wbce_dice_loss': wbce_dice_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/severstal-steel-defect-detection/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = DataGenerator(test, subset = 'test', batch_size = 2)\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2753/2753 [05:17<00:00,  8.67it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "#vanish = [5*700, 5*1300, 2*1000, 4*1300]\n",
    "vanish = [3000, 10000, 2000, 2*3000]\n",
    "THRESHOLD = 0.45\n",
    "sum = [0] * 4\n",
    "for i, batch in enumerate(tqdm(test_batches)): # index, (batch_size, 256, 1600, 3)\n",
    "    preds = model.predict_generator(batch) # массив из batch_size предсказаний, shape = (batch_size, 256, 1600, 4)\n",
    "    for k, img in enumerate(batch): # прохожу по каждому изображению из батча\n",
    "        mask = preds[k]\n",
    "        mask[mask >= THRESHOLD] = 1\n",
    "        mask[mask < THRESHOLD] = 0\n",
    "        for cls in range(4): # прохожу по каждому из 4 классов дефектов\n",
    "            sum[cls] = sum[cls] + np.sum(mask[:,:,cls])\n",
    "            if np.sum(mask[:,:,cls]) <= vanish[cls]:\n",
    "                mask[:,:,cls] = 0\n",
    "            rle = mask2rle(mask[:,:,cls])\n",
    "            name = test['ImageId'].iloc[i * batch_size + k] + f\"_{cls + 1}\"\n",
    "            predictions.append([name, rle])\n",
    "\n",
    "df = pd.DataFrame(predictions, columns=['ImageId_ClassId', 'EncodedPixels'])\n",
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55, 0, 1255, 307]\n",
      "22727.2\n",
      "inf\n",
      "23572.46374501992\n",
      "33384.84364820847\n"
     ]
    }
   ],
   "source": [
    "df1 = df[df['EncodedPixels'] != '']\n",
    "sum1 = 4 * [0]\n",
    "for i in range(df1.shape[0]):\n",
    "    if int(df1['ImageId_ClassId'].iloc[i].split('_')[1]) == 1:\n",
    "        sum1[0] = sum1[0] + 1\n",
    "    if int(df1['ImageId_ClassId'].iloc[i].split('_')[1]) == 2:\n",
    "        sum1[1] = sum1[1] + 1\n",
    "    if int(df1['ImageId_ClassId'].iloc[i].split('_')[1]) == 3:\n",
    "        sum1[2] = sum1[2] + 1\n",
    "    if int(df1['ImageId_ClassId'].iloc[i].split('_')[1]) == 4:\n",
    "        sum1[3] = sum1[3] + 1\n",
    "print(sum1)\n",
    "for i in range(4):\n",
    "    print(sum[i] / sum1[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
